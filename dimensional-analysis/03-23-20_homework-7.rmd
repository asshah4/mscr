---
title: "MSCR 509: High Dimensional Analysis"
subtitle: "Homework 7"
author: Anish Shah
date: March 23, 2020
output: pdf_document
latex_engine: xelatex
header-includes:
  - \usepackage{dcolumn}
  - \usepackage{float}
  - \usepackage{graphicx} 
  - \usepackage{booktabs}
  - \usepackage{longtable}
  - \usepackage{array}
  - \usepackage{multirow}
  - \usepackage{wrapfig}
  - \usepackage{colortbl}
  - \usepackage{pdflscape}
  - \usepackage{tabu}
  - \usepackage{threeparttable}
---

```{r setup, global_options, include=FALSE}
knitr::opts_chunk$set(
	cache = TRUE,
	warning = FALSE,
	eval = TRUE,
	echo = FALSE,
	include = FALSE,
	message = FALSE,
	dpi = 600,
	dev = "png",
	options("scipen" = 999, "digits" = 4)
)

library(knitr)
library(tinytex)
library(rmarkdown)
library(tidyverse)
library(broom)
library(haven)
library(magrittr)
library(lmtest)
library(compareGroups)
library(pROC)
library(stargazer)
library(caret)
library(e1071)
library(kableExtra)
library(ResourceSelection)
library(glmulti)
library(cvAUC)
library(glmnet)
library(MASS)
```

# Question 1

_The dataset (lasso_homework_prostate_tumor) contains prostate cancer tumor gene data. The original data contains 6033 genes. For this exercise we have selected 29 genes. The sample size contains 102 observations (men), whereof 52 have prostate tumor and 50 do not have prostate tumor.  The response variable Y is a binary variable that indicates the presence or absence of prostate tumor (cancer or healthy). The goal of the study is to build the model that correctly classifies the cancer and healthy observations._

## Part A

_Use logistic regression procedures (forward, backward) to develop a model for predicting cancer status.  Read the LOG window and decide if it is successful, explain the reasoning._

```{r}
# Data
genes <- read_sas("lasso_homework_prostate_tumor.sas7bdat")

# Clean
genes$y	%<>% factor(levels = c(1,2), labels = c(1,0))

# Create a regression with all variables
m <- glm(y ~ ., family = binomial("logit"), data = genes)

# Stepwise model
n <- stepAIC(m, direction = "both")
```

```{r, include = TRUE, results = 'asis'}
tidy(n, exponentiate = TRUE, confint = TRUE) %>%
	xtable(., display = rep('g', 6))
```

This is not a successful model. There are several variables that have an OR of almost 0, or an OR that is approximating infinity. The p-value for the "best" variables by stepwise regression are all very close to ~1. 

## Part B

_Using Lasso method develop a model to predict cancer vs normal subjects (Use correct coding in model statement to predict cancer... 1 = prostate cancer, 2 = no cancer)._

```{r}
# Create the y and x variables
x <- model.matrix(y ~ ., genes)[, -1]
y <- genes$y %>% as.character() %>% as.numeric()
lambda <- 10^seq(10, -2, length = 100)

# Training and test sets
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
ytest <- y[test]

# Model fit
m <- glm(y ~ ., family = binomial("logit"), data = genes)
coef(m)

# LASSO approach to identify optimal lambda
cvm <- cv.glmnet(x[train,], y[train], alpha = 1, lambda = lambda)
best_lam <- cvm$lambda.min

# Now, we retrain th elasso to fit
ml <- glmnet(x[train,], y[train], alpha = 1, lambda = best_lam)
pred <- predict(ml, s = best_lam, newx = x[test,])

```

The following covariates were found to be the most predictive using a LASSO method with a 50/50 test/training split. The optimal lambda was found to be `r best_lam`. The data as you can see has several variables that were shrunk to zero: _x332, x363, x914, x1113, x1130, x3665, x3991, x4546_.

```{r, include=TRUE, results='asis'}
xtable(as.matrix(coef(ml)))
```

# Question 2

_Logistic LASSO regression for the diagnosis of breast cancer using clinical demographic data and the BI-RADS lexicon for ultrasonography_

## Part A

_Briefly describe the goal of this article._

The goal of this article was to compare two different models in the evaluation of clinical characteristics into image analysis to improve breast cancer diagnosis. 

## Part B

_Describe the statistical methods and results._

The researchers used a stepwise LR and a least absolute shrinkage and selection operator (LASSO) regression. The BIRADS score and teh clinical factors were used as the covariates. The models were fit with training data only, and then used for prediction on the test data. They used cross-validation for lambda selection. The BIRADS score, generated by radiologists, were pooled, with a majority agreement leading to a positive diagnosis. 

The LASSO approach resulted in a misclassification rate of 0.234 versus 0.253 by stepwise regression. Using clinical covariates improved misclassification errors (from 0.194 to 0.234). Their overall LASSO approach was similar to that of the AUC of radiologists with BIRADS alone.  

## Part C

_Based on this article, what are the advantages of the LASSO approach?_

The advantages of LASSO are that it allows the inclusion of multiple covariates of unknown importance, and with its shrinkage methods it allows for the retention of only the important covariates. It outperforms stepwise regression, which can get "stuck" on non-informative parameters. 
